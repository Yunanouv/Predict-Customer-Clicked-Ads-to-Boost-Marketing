# -*- coding: utf-8 -*-
"""MP-Predict Customer Clicked Ads.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JvRWQqIUsSj-uqWo6iqLAIASjx5qKz_3
"""

!pip install dython

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import datetime
from sklearn.preprocessing import MinMaxScaler
import plotly.figure_factory as ff
import plotly.graph_objects as go

# %matplotlib inline

dfraw = pd.read_csv('https://drive.google.com/uc?export=download&id=1J20q-aMSGYwIfpkUoULcFN_ZSCBNuZUe', low_memory=False)
dfraw.info()
dfraw.sample(5)

"""Keterangan :  
1. Kolom `Unnamed: 0` dapat dihapus karena berisikan nomor index  
2. Terdapat beberapa fitur yang berisikan null value yaitu `Daily Time Spent on Site`, `Area Income`, `Daily Internet Usage`, `Male`.  
3. Fitur 'Male` perlu diubah nama nya menjadi `Gender` karena berisikan value jenis kelamin  
4. Fitur `Timestamp` perlu diubah ke tipe data datetime.  
"""

dfraw.drop(columns='Unnamed: 0', inplace = True)
dfraw = dfraw.rename(columns= {'Male':'Gender'})
dfraw['Timestamp'] = pd.to_datetime(dfraw['Timestamp'], format='%m/%d/%Y %H:%M')

# Check duplicated rows
dfraw.duplicated().sum()

# Columns
nums = dfraw.select_dtypes(exclude='object').columns.tolist()
cats = dfraw.select_dtypes(include='object').columns.tolist()
dfraw[nums].describe().T

dfraw[cats].describe().T

"""# EDA"""

dfraw['city'].unique()

dfraw['province'].unique()

dfraw['category'].unique()

plt.figure(figsize=(15, 3))
for i in range(0, len(nums)):
    plt.subplot(1, len(nums), i+1)
    sns.distplot(dfraw[nums[i]], color='red')
    plt.tight_layout()
plt.savefig('Univariate Analysis', dpi = 200)

# Outliers
f_out = ['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage']
plt.figure(figsize=(6, 3))
for i in range(0, len(f_out)):
    plt.subplot(1, len(f_out), i+1)
    sns.boxplot(dfraw[f_out[i]], color='red', orient='v')
    plt.title(f_out[i])
    plt.tight_layout()

#get the info of the number of ad clicked
fig = plt.figure(figsize = (15, 3))
sns.countplot(x ='Age', data = dfraw)

# Making new list without timestamp
f = ['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage']
plt.figure(figsize=(12,5))
for i in range(0,len(f)):
    plt.subplot(1,len(f),i+1)
    sns.boxenplot(x=dfraw['Clicked on Ad'], y=dfraw[f[i]], palette = ['#4361EE', '#D90429'])
    plt.xlabel('Clicked on Ad', fontsize=14)
    plt.ylabel(f[i], fontsize=14)
    plt.tick_params(axis='both', which = 'major', labelsize=14)
    plt.tight_layout()
plt.suptitle('Bivariate Analysis', fontweight='bold', fontsize=20, y=1.03)
plt.savefig('Bivariate Analysis', dpi = 200)
plt.tight_layout()
plt.show()

# Correlation between daily internet usage and time spent on site
sns.set_theme(style="white")
f, ax = plt.subplots(figsize=(7, 6))
sns.kdeplot(data = dfraw, x='Daily Time Spent on Site', y='Daily Internet Usage', n_levels=15, fill=True, legend=True,
            bw_method = 'scott', bw_adjust =1, cut=2, palette=['#4361EE', '#D90429'], hue='Clicked on Ad');
plt.savefig('Corr-Daily Internet Usage-Time Spent on Site', dpi = 200)

# Correlation Matrix Heatmap
import matplotlib
from dython.nominal import associations
fig, ax = plt.subplots(figsize=(9, 8))
plt.tick_params(axis='both', which='major', labelsize=8)
matplotlib.rc('xtick', labelsize=8)
matplotlib.rc('ytick', labelsize=8)
plt.rcParams.update({'font.size': 8})
associations(dfraw, ax=ax, plot=False);
plt.savefig('Corr Matrix Heatmap', dpi = 200)

sns.jointplot(x='Area Income',y='Daily Internet Usage', data=dfraw, hue="Clicked on Ad", palette='rocket')

"""# Data Cleaning

## Handling Missing Values
"""

# View number of missing values
null_cols = dfraw.columns[dfraw.isnull().any()]
df_null = dfraw[null_cols].isnull().sum().to_frame().reset_index()
df_null.columns = ['Feature', 'Counts']
df_null['Percentage(%)'] = round(df_null['Counts']/len(dfraw) * 100, 3)
df_null

# Handling missing values
ncols = ['Daily Time Spent on Site', 'Area Income', 'Daily Internet Usage']
dfraw[ncols] = dfraw[ncols].fillna(dfraw[ncols].median())

dfraw['Gender'] = dfraw['Gender'].fillna(dfraw['Gender'].mode()[0])

print(f'Is still there null values? {dfraw.isnull().any().any()}')

"""## Feature Extraction"""

dfcleaned = dfraw.copy()

dfcleaned['Year'] = dfcleaned['Timestamp'].dt.year
dfcleaned['Month'] = dfcleaned['Timestamp'].dt.month
dfcleaned['Week'] = dfcleaned['Timestamp'].dt.isocalendar().week
dfcleaned['Day'] = dfcleaned['Timestamp'].dt.day

dfcleaned.sample(5)

# Shorten unique value
dfcleaned['province'] = np.where(dfcleaned['province']=='Daerah Khusus Ibukota Jakarta', 'DKI Jakarta',
                                 np.where(dfcleaned['province']=='Jawa Barat', 'Jawa Barat', 'Lainnya'))

"""## Feature Encoding"""

cats_encode = ['Gender','Clicked on Ad','city','province','category']

for col in cats_encode:
  print(f'value counts of column {col}')
  print(dfcleaned[col].value_counts())
  print('---'*10, '\n')

# Label Encoding for `Clicked on Ad`
dfcleaned['Clicked on Ad'] = dfcleaned['Clicked on Ad'].replace({'No':0, 'Yes':1})

# OHE for Gender, Province, and Category
dfcleaned = pd.get_dummies(dfcleaned, columns=['Gender', 'province', 'category'])

"""## Feature Selection"""

dfselect = dfcleaned.copy()
dfselect = dfselect.drop(columns = ['city', 'Timestamp'])
dfselect.sample(3)

"""# Modeling"""

dfselect.info()

from sklearn.model_selection import train_test_split

# Split the data
features = dfselect.select_dtypes(["float64", "int64", "uint8"]).columns
x = dfselect[features].drop('Clicked on Ad', axis=1)
y = dfselect['Clicked on Ad']

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=42)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

"""#### Modeling w/o Scaling"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, recall_score, precision_score
from sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay
from collections import defaultdict

from warnings import filterwarnings
filterwarnings('ignore')

# Modeling without scaling
def mymodel(xtrain,xtest,ytrain,ytest):
  result = defaultdict(list)

  KNN = KNeighborsClassifier()
  LR = LogisticRegression()
  DTC = DecisionTreeClassifier()
  RFC = RandomForestClassifier()
  XGB = XGBClassifier()

  list_model = [('K-Nearest Neighbor',KNN),
                  ('Logistic Regression',LR),
                  ('Decision Tree',DTC),
                  ('Random Forest',RFC),
                  ('XGB',XGB)]

  for model_name, model in list_model:
    model.fit(xtrain,ytrain)
    ypred = model.predict(xtest)

    accuracy = accuracy_score(ytest,ypred)
    recall = recall_score(ytest,ypred)
    precision = precision_score(ytest,ypred)
    result['model_name'].append(model_name)
    result['model'].append(model)
    result['accuracy'].append(accuracy)
    result['recall'].append(recall)
    result['precision'].append(precision)

  return result

print('\nModeling Without Feature Scaling')
result = mymodel(xtrain,xtest,ytrain,ytest)
result = pd.DataFrame(result)
result

"""## Modeling w/ Scaling"""

from sklearn.preprocessing import MinMaxScaler
minmax_scaler = MinMaxScaler()
xtrain_scaled = minmax_scaler.fit_transform(xtrain)
xtest_scaled = minmax_scaler.transform(xtest)

print('\nModeling With Feature Scaling')
result2 = mymodel(xtrain_scaled,xtest_scaled,ytrain,ytest)
result2 = pd.DataFrame(result2)
result2

# Confusion Matrix Logistic Regression

model_LR = result2['model'][1]
predictions_LR = model_LR.predict(xtest_scaled)
cf_matrix = confusion_matrix(predictions_LR, ytest)

group_names = ['TN','FP','FN','TP']
group_counts = ['{0:0.0f}'.format(value) for value in
                cf_matrix.flatten()]
group_percentages = ['{0:.2%}'.format(value) for value in
                     cf_matrix.flatten()/np.sum(cf_matrix)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)

fig, ax = plt.subplots(figsize=(5, 4))
plt.tick_params(axis='both', which='major', labelsize=8)
plt.rcParams.update({'font.size': 8})
ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')

plt.title('Logistic Regression\nConfusion Matrix', pad=15)
plt.xlabel('Predicted Values')
plt.ylabel('Actual Values ');
plt.savefig('LogisticReg ConfMtrx', dpi = 200)

# Confusion Matrix Logistic Regression

model_RF = result2['model'][3]
predictions_RF = model_RF.predict(xtest_scaled)
cf_matrix = confusion_matrix(predictions_RF, ytest)

group_names = ['TN','FP','FN','TP']
group_counts = ['{0:0.0f}'.format(value) for value in
                cf_matrix.flatten()]
group_percentages = ['{0:.2%}'.format(value) for value in
                     cf_matrix.flatten()/np.sum(cf_matrix)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)

fig, ax = plt.subplots(figsize=(5, 4))
plt.tick_params(axis='both', which='major', labelsize=8)
plt.rcParams.update({'font.size': 8})
ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Greens')

plt.title('Random Forest\nConfusion Matrix', pad=15)
plt.xlabel('Predicted Values')
plt.ylabel('Actual Values ');
plt.savefig('RandomF ConfMtrx', dpi = 200)

# Confusion Matrix Logistic Regression

model_XGB = result2['model'][4]
predictions_XGB = model_XGB.predict(xtest_scaled)
cf_matrix = confusion_matrix(predictions_XGB, ytest)

group_names = ['TN','FP','FN','TP']
group_counts = ['{0:0.0f}'.format(value) for value in
                cf_matrix.flatten()]
group_percentages = ['{0:.2%}'.format(value) for value in
                     cf_matrix.flatten()/np.sum(cf_matrix)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)

fig, ax = plt.subplots(figsize=(5, 4))
plt.tick_params(axis='both', which='major', labelsize=8)
plt.rcParams.update({'font.size': 8})
ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Reds')

plt.title('XGB\nConfusion Matrix', pad=15)
plt.xlabel('Predicted Values')
plt.ylabel('Actual Values ');
plt.savefig('XGB ConfMtrx', dpi = 200)

"""The best model is Random Forest. High TP and TP, and low FP."""

pip install shap

import shap

# Menampilkan summary shap
explainer = shap.TreeExplainer(model_RF)
shap_values = explainer.shap_values(xtest)
shap.summary_plot(shap_values, xtest)

"""# Business Recommendation"""

df = dfraw.copy()

# Age Grouping

def age_group(x):
    if x <= 30:
        return '<30'
    elif x <= 40:
        return '31-40'
    elif x <= 50:
        return '41-50'
    else:
        return '> 50'

df['Age_Group'] = df['Age'].apply(lambda x : age_group(x))

sns.set(style='darkgrid')
fig, ax = plt.subplots(figsize=(8, 5))
plt.title("Group of Age", fontsize=15, color='black', weight='bold', pad=45)
sns.countplot(x='Age_Group', data=df, hue='Clicked on Ad', palette=['#D00000', '#FFF3B0'])

plt.tick_params(axis='both', which='major', labelsize=9)
plt.grid()
plt.legend(title='Clicked on Ad', title_fontsize=9, prop={'size':8})
plt.xlabel('Age (years old)', fontsize=11)
plt.xticks(range(0,4,1), labels=['<30', '31-40', '41-50', '>50'], fontsize=10)
plt.ylim(0, 300)
plt.bar_label(ax.containers[0], padding=5, fontsize=8)
plt.bar_label(ax.containers[1], padding=2, fontsize=8)

sns.despine()
plt.tight_layout()
#plt.savefig('grup_age.png')

dfraw['Daily Internet Usage'].describe()

from re import X
# Daily Internet Usage

internet_group = []
for x in df['Daily Internet Usage']:
    if x >= 100 and x <= 140:
        group = '100-140'
    elif x >= 141 and x <= 180:
        group = '141-180'
    elif x >= 181 and x <= 220:
        group = '181-220'
    elif x >= 221 and x <= 250:
        group = '221-250'
    else:
        group = '> 250'
    internet_group.append(group)

df['Internet_Group'] = internet_group

sns.set(style='whitegrid')
fig, ax = plt.subplots(figsize=(8, 5))
plt.title("Daily Internet Usage Group", fontsize=15, color='black', weight='bold', pad=45)
sns.countplot(x='Internet_Group', data=df, hue='Clicked on Ad', edgecolor= 'pink', palette=['#774936', '#FFB3C1'])

plt.tick_params(axis='both', which='major', labelsize=9)
plt.grid()
plt.legend(title='Clicked on Ad', title_fontsize=9, prop={'size':8})
plt.xlabel('Daily Internet Usage (minute)', fontsize=11)
plt.xticks(range(0,5,1), labels=['100-140', '141-180', '181-220', '221-250', '>250'], fontsize=10)
plt.ylim(0, 300)
plt.bar_label(ax.containers[0], padding=5, fontsize=8)
plt.bar_label(ax.containers[1], padding=2, fontsize=8)

sns.despine()
plt.tight_layout()
#plt.savefig('grup_Internet.png')

